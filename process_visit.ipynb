{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dataSchema = StructType(StructField(HN,StringType,false), StructField(DISEASE,IntegerType,false), StructField(DATEDEFINE,DateType,false), StructField(TYPE,IntegerType,false), StructField(ICD10,StringType,false), StructField(PATIENT_LOCATION_CODE,StringType,false))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "StructType(StructField(HN,StringType,false), StructField(DISEASE,IntegerType,false), StructField(DATEDEFINE,DateType,false), StructField(TYPE,IntegerType,false), StructField(ICD10,StringType,false), StructField(PATIENT_LOCATION_CODE,StringType,false))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types.{DateType, IntegerType, StringType, StructField, StructType}\n",
    "\n",
    "val dataSchema = new StructType(Array(\n",
    "    new StructField(\"HN\", StringType, false),\n",
    "    new StructField(\"DISEASE\", IntegerType, false),\n",
    "    new StructField(\"DATEDEFINE\", DateType, false),\n",
    "    new StructField(\"TYPE\", IntegerType, false),\n",
    "    new StructField(\"ICD10\", StringType, false),\n",
    "    new StructField(\"PATIENT_LOCATION_CODE\", StringType, false)\n",
    "  ));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "source = [HN: string, DISEASE: int ... 4 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[HN: string, DISEASE: int ... 4 more fields]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val source = spark.read.format(\"csv\")\n",
    "  .option(\"mode\", \"FAILFAST\")\n",
    "  .option(\"inferSchema\", \"true\")\n",
    "  .option(\"header\", \"true\")\n",
    "  .schema(dataSchema)\n",
    "  .option(\"path\", \"./hos_visits.csv\")\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "flu = UserDefinedFunction(<function1>,IntegerType,Some(List(ArrayType(StringType,true))))\n",
       "pneumonia = UserDefinedFunction(<function1>,IntegerType,Some(List(ArrayType(StringType,true))))\n",
       "ili = UserDefinedFunction(<function2>,IntegerType,Some(List(ArrayType(StringType,true), IntegerType)))\n",
       "sari = UserDefinedFunction(<function2>,IntegerType,Some(List(ArrayType(StringType,true), IntegerType)))\n",
       "dfd = [HN: string, DISEASE: int ... 8 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[HN: string, DISEASE: int ... 8 more fields]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "import scala.collection.mutable\n",
    "\n",
    "\n",
    "val flu = udf((x: mutable.WrappedArray[String]) => {\n",
    "    if (x.exists((s) => s.startsWith(\"J10\") || s.startsWith(\"J11\"))) 1 else 0\n",
    "})\n",
    "\n",
    "val pneumonia = udf((ary: mutable.WrappedArray[String]) => {\n",
    "    if (ary.exists(s => s.matches(raw\"^J1[2345678]\") || s.startsWith(\"J85\"))) 1 else 0\n",
    "  })\n",
    "\n",
    "val ili = udf((ary: mutable.WrappedArray[String], patientType: Int) => {\n",
    "    if (patientType == 1) { // OPD only\n",
    "      //J00, J02.9, J06.9, J09,J10,J11\n",
    "      if (ary.exists(s => s.matches(raw\"^(J00|J029|J069|J09|J10|J11)\"))) 1 else 0\n",
    "    } else 0\n",
    "  })\n",
    "\n",
    "val sari = udf((ary: mutable.WrappedArray[String], patientType: Int) => {\n",
    "    if (patientType == 2) { // IPD only\n",
    "      // J00-J22\n",
    "      if (ary.exists(s => s.matches(raw\"^J[01][0-9]\") || s.matches(raw\"^J2[0-2]\"))) 1 else 0\n",
    "    } else 0\n",
    "  })\n",
    "\n",
    "val dfd = source\n",
    "    .withColumn(\"ICD10\", split(col(\"ICD10\"), \"\\\\|\"))\n",
    "    .withColumn(\"flu\", flu(col(\"ICD10\")))\n",
    "    .withColumn(\"pnuemonia\", pneumonia(col(\"ICD10\")))\n",
    "    .withColumn(\"ili\", ili(col(\"ICD10\"), col(\"TYPE\")))\n",
    "    .withColumn(\"sari\", sari(col(\"ICD10\"), col(\"TYPE\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+----------+----+------------------+---------------------+---+---------+---+----+\n",
      "|       HN|DISEASE|DATEDEFINE|TYPE|             ICD10|PATIENT_LOCATION_CODE|flu|pnuemonia|ili|sari|\n",
      "+---------+-------+----------+----+------------------+---------------------+---+---------+---+----+\n",
      "|000019279|     26|2020-01-01|   1|[A919, , , , , , ]|               130608|  0|        0|  0|   0|\n",
      "+---------+-------+----------+----+------------------+---------------------+---+---------+---+----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfd.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "summary = [DATEDEFINE: date, PATIENT_LOCATION_CODE: string ... 4 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[DATEDEFINE: date, PATIENT_LOCATION_CODE: string ... 4 more fields]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val summary = dfd.groupBy($\"DATEDEFINE\", $\"PATIENT_LOCATION_CODE\")\n",
    "    .agg(sum($\"flu\") as \"flu\"\n",
    "         ,sum($\"ili\") as \"ILI\"\n",
    "         ,sum($\"sari\") as \"SARI\"\n",
    "         ,sum($\"pnuemonia\") as \"pnuemonia\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------------+---+---+----+---------+\n",
      "|DATEDEFINE|PATIENT_LOCATION_CODE|flu|ILI|SARI|pnuemonia|\n",
      "+----------+---------------------+---+---+----+---------+\n",
      "|2020-01-02|               104601|  1|  0|   0|        0|\n",
      "|2020-01-02|               130601|  1|  0|   0|        0|\n",
      "|2020-01-03|               130605|  0|  0|   0|        0|\n",
      "|2020-01-03|               130607|  0|  0|   0|        0|\n",
      "|2020-01-05|               130603|  0|  0|   0|        0|\n",
      "|2020-01-05|               130607|  0|  0|   0|        0|\n",
      "|2020-01-06|               130603|  0|  0|   0|        0|\n",
      "|2020-01-06|               170106|  0|  0|   0|        0|\n",
      "|2020-01-06|               451602|  0|  0|   0|        0|\n",
      "|2020-01-07|               130606|  0|  0|   0|        0|\n",
      "|2020-01-07|               130604|  0|  0|   0|        0|\n",
      "|2020-01-07|               670603|  0|  0|   0|        0|\n",
      "|2020-01-09|               130305|  1|  0|   0|        0|\n",
      "|2020-01-09|               130601|  0|  0|   0|        0|\n",
      "|2020-01-10|               130608|  0|  0|   0|        0|\n",
      "|2020-01-10|               130101|  0|  0|   0|        0|\n",
      "|2020-01-11|               130606|  0|  0|   0|        0|\n",
      "|2020-01-12|               101006|  0|  0|   0|        0|\n",
      "|2020-01-12|               412203|  0|  0|   0|        0|\n",
      "|2020-01-12|               100303|  0|  0|   0|        0|\n",
      "+----------+---------------------+---+---+----+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summary.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary.repartition(1)\n",
    "    .write\n",
    "    .format(\"csv\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"path\", \"./summary_single.csv\")\n",
    "    .option(\"header\", \"TRUE\")\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
